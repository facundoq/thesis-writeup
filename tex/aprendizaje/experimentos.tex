
Una vez entrenado un clasificador, típicamente se quiere conocer que tan bien clasifica a los ejemplares del dominio del problema a resolver. Para evaluar un clasificador generalmente se realizan los siguientes pasos:

\begin{itemize}
	\item Obtener los datos de los ejemplares, preprocesarlos \footnote{El preprocesamiento involucra limpiar los ejemplos, quitando casos anómalos, ruido, errores de los mismos, y en ocasiones llevar los mismos a una forma canónica o normalizarlos.}, calcular las características correspondientes.
  \item Seleccionar los parámetros del clasificador. \footnote{Por ejemplo, si el clasificador es un perceptrón, se debe seleccionar la tasa de aprendizaje.}
  \item Entrenar un clasificador con los ejemplares y algún algoritmo de entrenamiento.
  \item Probar el modelo generado obteniendo alguna medida de error (las siguientes secciones discutirán varias formas de hacer esto).
\end{itemize}

Se llamará a esta serie de pasos un \textbf{experimento}. El resultado del experimento es el modelo entrenado y una medida de error del mismo respecto al dominio $\ddp$ y la asignación de etiquetas $y_i$ para los ejemplares.

El objetivo del experimento es el último punto, la evaluación del desempeño del modelo. Para ello, se pensará en el experimento como un experimento estadístico, que estimará el desempeño del modelo dado el dominio $\ddp$ y la asignación de 
etiquetas $y_i$.

Como en toda prueba estadística, interesará el \textit{comportamiento promedio} del clasificador, y poder caracterizar su variabilidad.

\subsubsection{Fuentes de variabilidad}

Un experimento de clasificación tiene dos fuentes principales de variabilidad o aleatoriedad. 

La primera está dada por los datos utilizados para entrenar y probar el clasificador. Distintos datos para entrenar generan modelos distintos, y por ende el error distinto. De la misma forma, usar datos distintos para probar el clasificador hará que varíe el valor del estimador calculado. En general, cuantos más datos para entrenar pueda usar el algoritmo de entrenamiento y mejor sea su calidad (representatividad de la variabilidad del dominio del problema, poco error de medición), mejor será el modelo generado, por las mismas razones que el ajuste a una curva en base a puntos de ejemplo de la misma es más preciso con más puntos. 

De la misma forma, cuanto más y mejores datos se utilicen para evaluar el error del clasificador mejor será la confianza de las pruebas, por las mismas razones que una muestra grande de datos es deseable para realizar un test estadístico.

La segunda se encuentra en la generación del clasificador. Muchos algoritmos de entrenamiento utilizan el enfoque de partir de un estado generado aleatoriamente, y luego mejorarlo iterativamente durante el entrenamiento \footnote{El algoritmo backpropagation descripto en el capítulo \ref{chap:neuronales} de redes neuronales posee esta característica.}. La inicialización es aleatoria, y como el estado final depende del estado inicial, también es aleatorio el error del clasificador. En otros casos, el estado inicial puede estar fijado de antemano, pero los pasos para llegar al estado final contienen elementos aleatorios.

Si bien esta fuente de aleatoriedad puede eliminarse utilizando una semilla fija en la generación de números aleatorios, dicha estrategia impediría el análisis de una serie de experimentos con herramientas estadísticas, lo cual es necesario para obtener una medida de error del clasificador en promedio realizando varios experimentos y agregando los resultados. 

El concepto de experimento será útil para hacer referencia a esta serie de pasos en las siguientes secciones. El objetivo de un experimento es generar y evaluar un clasificador. En la evaluación, interesa esencialmente el comportamiento del clasificador entrenado en \textit{nuevos} ejemplares del problema, no vistos en la etapa de entrenamiento; este es el problema de la \textbf{generalización}.