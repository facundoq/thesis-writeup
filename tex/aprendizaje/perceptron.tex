
A modo de ejemplo, se puede considerar un tipo de clasificador muy simple y emblemático, el \textbf{Perceptrón} \cite{rosenblatt1958}. Si bien el mismo tiene origen en el campo particular de las redes neuronales dentro del aprendizaje automático, puede comprenderse solamente en términos geométricos. 

El Perceptrón es arquetípico en aprendizaje automático porque es un tipo de clasificador que divide ejemplares de dos clases bajo la asunción de que existe un \textbf{hiperplano} en $\ddp$, $\wv \xv +b=0$ que separa las dos clases; es decir, los ejemplares de cada clase se encuentran en lados opuestos del hiperplano \footnote{Un hiperplano es la generalización del concepto de punto en $\reals^1$, de línea en $\reals^2$, y de plano en $\reals^3$.}.

El vector $\wv$ es el vector normal o perpendicular al hiperplano, y determina la dirección en la que se propaga el hiperplano en cada dimensión.

El bias $b$ permite realizar traslaciones verticales del hiperplano, de manera que no esté obligado a pasar por el origen, y por ende aumenta su poder para dividir el espacio en dos de forma arbitraria, y así clasificar mejor.

En este caso al tener solo dos clases se asumirá que $y=\pm 1$; los ejemplares con clase positiva $+1$ se encuentran del lado a donde apunta el vector normal al hiperplano, $\wv \in \reals^d$. Los ejemplares con clase negativa $-1$ se encuentran del lado opuesto. \footnote{Los hiperplanos afines están dados por la ecuación $\hiper$, los hiperplanos lineales por $\hiperlineal$}

\tikzimage{hiperplano_perceptron}{scale=0.25}{Hiperplano separador de los ejemplos de dos clases.}


Entonces, un Perceptrón está dado por un par $(\wv,b)$; un algoritmo de entrenamiento determina esos valores de acuerdo a un conjunto de datos $D=\left\lbrace \xi \right\rbrace$ y a las clases de los ejemplares $y_i$. 

La idea esencial del algoritmo de entrenamiento del Perceptrón es, dado un conjunto de datos $D$ como el descripto más arriba, generar un hiperplano inicial de alguna manera y cambiarlo o moverlo (con algún criterio) hasta que clasifique bien todos los ejemplares.


%\tikzimage{hiperplanoperceptronentrenamiento}{Mejora paulatina del hiperplano como separador de las clases.}{scale=0.25}
\image{perceptron_entrenamiento}{0.4}{Mejora paulatina del hiperplano como separador de las clases.}

Para comenzar, se puede generar un hiperplano inicial donde $\wv$ y $b$ tienen valores aleatorios. 

Luego, se toma un ejemplar $\xi$ del conjunto $D$ y se verifica si el modelo lo clasifica correctamente. Si es así, se vuelve a elegir otro ejemplar; sino, se rota el hiperplano ligeramente de manera que el ejemplar $\xi$ quede más cerca del otro lado del hiperplano. 

Este procedimiento se repite hasta que todos los ejemplares se encuentran en el lado correcto del hiperplano, es decir, sean clasificados correctamente. En pseudocódigo:

\begin{algorithm}[H]
\KwData{ Un conjunto de ejemplares $D \in \reals^d$\\
Una clase $y_i \in \{-1,1\}$ asociada a cada $\xi \in D$}
\KwResult{Un hiperplano descripto por $\wv \in \reals^{d}$, un bias $b \in \reals$}
$\wv=random(d)$\;
$b=random(1)$\;
\While{haya ejemplares que $\wv$ y $b$ no clasifiquen correctamente}{ 
  \For{$\xi \in D$}{
      \If{$\wv$ y $b$ no clasifican bien a $\xi$}{
        Modificar $\wv$ y $b$ de manera que $\xi$ esté más cerca del lado correcto \;
      }  
  }    
}
\caption{Esquema del algoritmo de aprendizaje del Perceptrón} 
\end{algorithm}
\vspace{10pt}

Para formalizar las nociones de ``clasifica bien'' y ``modificar $\wv$ y $b$'', se derivará:

\begin{itemize}
\item Una función $f$ de clasificación
\item Un predicado $p$ que sea cierto solo si todos los ejemplares están clasificados correctamente
\item Una regla de actualización o aprendizaje para $\wv$ y $b$, tal que su aplicación repetida logre que el hiperplano mejore su clasificación de los ejemaplres.
\end{itemize} 

\subsubsection{Derivación de $f$}

Para facilitar la notación, se define una función $h$ que tiene como parámetros el vector normal $\wv \in \reals^{d}$ y el \textit{bias} $b$:

\begin{equation}
 h(\xv)= \wv \cdot \xv + b 
 \end{equation}

De manera que el hiperplano está definido como $h(\xv)=0$, y\footnote{Por conveniencia, si bien $h$ es una función y no una ecuación, la expresión \textit{el hiperplano $h$ }, en realidad hace referencia al hiperplano $h(\xv)=0$.}:

\begin{itemize}
\item Para los ejemplares $\xv$ sobre el hiperplano, vale que $h(\xv)=0$
\item Para los ejemplares $\xv$ del lado del hiperplano a donde apunta el vector normal, vale $h(\xv)>0$
\item Para los ejemplares $\xv$ del lado del hiperplano \textbf{opuesto} a donde apunta al vector normal, $h(\xv)<0$
\end{itemize}


Para clasificar un ejemplar $\xv$, es necesaria entonces una función $f$ tal que :
\begin{equation}
 f(\xv)= \case{1}{-1}{ \text{la clase de $\xv$ es $1$} }{ \text{la clase de $\xv$ es $-1$} }
 \end{equation}

Para realizar la transformación necesaria, se utiliza la función escalón o \textbf{step function} $\theta$:
\begin{equation}
\theta(x) = \caseotherwise{1}{-1}{x> 0}
 \end{equation}

\tikzimage{stepfunction}{scale=0.5}{Función $\theta$, comúnmente llamada \textit{step function} o función escalón.
}

Y la función $f$ toma la forma:

\ma{
 f(\xv)= \theta(h(\xv))
}

Donde $h$ indica con su signo de qué lado del hiperplano se encuentra el ejemplar $\xv$, y $\theta$ le asigna la clase correspondiente en base al signo.

\subsubsection{Predicado de clasificación correcta}

Utilizando la función $f$, se puede expresar de forma simple que no hay ejemplares $\xi$ clasificados incorrectamente cuando $f$ le asigna a cada uno su clase correspondiente $y_i$, definiendo :

\begin{equation}
p \equiv \forall \xi \in D, \; f(\xi) = y_i
\end{equation}

\subsubsection{Regla de actualización de $\wv$ y $b$ }

Para la modificación, se asumirá $b=0$ momentáneamente, es decir, se asume que $h$ es un hiperplano lineal, que pasa por el origen. El mismo está ahora dado por $h(\xv)=\wv \cdot \xv$.

A continuación, se deducirá una regla para cambiar a $\wv$ de manera que dado un $\xv$ clasificado incorrectamente se pueda calcular un $\wv'$ que tenga menos error que $\wv$ con respecto a $\xv$. 

Para eso, dado que $b=0$, si $\alpha$ es el ``ángulo'' entre $\xv$ y $\wv$, entonces de acuerdo con la definición geométrica del producto punto $\cdot$, $h(\xv)=\wv \cdot \xv= \norm{\wv} \norm{\xv} cos(\alpha)$. Por ende, el signo de $h$ es el signo de $cos(\alpha)$. Si $\alpha \in (-90,90) \rightarrow h>0$ y si $\theta \in (90,270) \rightarrow h < 0 $. 

\tikzimagetwo{hiperplano_regiones}{scale=0.25}{Regiones positivas y negativas para un hiperplano lineal $\wv \cdot \xv =0$ en $\reals^2$ (con $b=0$)}{hiperplano_angulos}{scale=0.25}{Perspectiva trigonométrica del signo de $\wv \cdot \xv$}

Se puede utilizar esta relación para derivar la regla de actualización. Dado un ejemplar $\xvp$, con clase $y^{+}=1$. Si $h(\xvp)=-1$, el ejemplar está clasificado incorrectamente. 

\tikzimage{hiperplano_mal_positivo}{scale=0.25}{Un ejemplar $\xvp$ mal clasificado.
}

De forma tentativa, se puede definir una regla simple de modificación de $\wv$ para arreglar dicho error: asignar $\wv \leftarrow \xvp$. De esta manera, claramente $\wv \cdot \xvp>0 $ y entonces el patrón se clasifica correctamente \footnote{Asumiendo $\xv_0 \neq \ve{0}$}. 

\tikzimage{hiperplano_mal_positivo_actualizado}{scale=0.25}{El ejemplar $\xvp$ es clasificado correctamente, pero se ha perdido la clasificación correcta de otros ejemplares.}

Esta idea también funciona con $\xvm$ tal que $y^{-}=-1$ y $h(\xvm)=1$, donde el problema es que $\xvm$ está del mismo lado que la normal $\wv$.

\tikzimage{hiperplano_mal_negativo}{scale=0.25}{Un ejemplar $\xvm$ mal clasificado.
}

En este caso, se asigna $\wv \leftarrow -\xvm$ y así $\wv \cdot \xvm = -\xvm \cdot \xvm <0$.

\tikzimage{hiperplano_mal_negativo_actualizado}{scale=0.25}{El ejemplar $\xvm$ es clasificado correctamente, pero se ha perdido la clasificación correcta de otros ejemplares.}

En cierto sentido, de esta forma se elige el $\wv$ óptimo para cada $\xv$ por separado, ya que el ejemplar queda perpendicular al hiperplano y por ende se maximiza la distancia desde $\xv$ hasta este último. Pero de esta manera para cada $\xv$ se hace un cambio muy extremo. Si se repite esto para todos los ejemplares, $\wv$ se irá adaptando a cada uno en particular, olvidando completamente los hiperplanos anteriores. Con esta regla de modificación solo se consigue clasificar correctamente a todos los ejemplares si por casualidad algún $\xv$ es la normal de un hiperplano adecuado para ello.

Se necesita una regla que sea más conservadora con los cambios, que modifique $\wv$ para que tenga menos error respecto a un $\xv$, pero que no descarte todo el progreso hecho en las iteraciones anteriores.

El algoritmo para entrenar un Perceptrón propone corregir $\wv$, dado un ejemplar mal clasificado $\xv$ de clase $y$, con una regla del tipo:

\begin{equation}
\wv \leftarrow \case{\wv+\xv}{\wv-\xv}{ y=1}{y=-1}
\end{equation}

La idea es que en vez de reemplazar $\wv$ directamente por $\xv$ o $-\xv$, $\wv$ se convierte de a poco en los vectores $\xv$. Se puede pensar en dicha conversión de la siguiente manera. Si $\wv_{\lambda}= \wv + \lambda \xvp$, a medida que $\lambda \rightarrow \infty$, $\wv_{\lambda}$ apunta cada vez más en la dirección de $\xvp$, ya que en comparación $ \norm{\wv} \ll \norm{\lambda \xvp}$. Se habla de dirección ya que $\lambda \xvp$ representa el mismo hiperplano para todo $\lambda \in \reals$, por ende no importa que $\norm{\wv_{\lambda}}$ también se haga arbitrariamente grande. Entonces, en cada iteración el algoritmo toma un ejemplar mal clasificado y ``acerca'' o ``aleja'' $\wv$ a dicho ejemplar.
 
Para simplificar dicha regla de actualización eliminando los casos, se puede aprovechar el hecho de que $y$ dice con su signo justamente si hay que sumar o restar $\xv$:

\begin{equation}
\wv \leftarrow \wv + y \xv
\end{equation}

También se puede controlar la magnitud de las correcciones introduciendo una \textbf{tasa de aprendizaje} $\alpha \in \reals$:

\begin{equation}
\wv \leftarrow \wv +  \alpha y \xv
\end{equation}

Si $\alpha \approx 0$, $\wv$ se cambia poco en cada iteración y el aprendizaje es muy lento pero seguro, ya que recorre el espacio de posibles $\wv$ minuciosamente. Si $\alpha \gg 0$, $\wv$ se cambia mucho en cada iteración y por ende el comportamiento del entrenamiento se vuelve más errático, similar al de la regla $\wv \leftarrow \xv$; se debe encontrar un punto medio para dicho valor, dependiente de cada problema a resolver.

\subsubsection{Re-introducción del bias $b$}

Por último, para esta derivación se asumió que $b=0$, o sea, que el hiperplano era lineal, pasando por el origen de coordenadas. Para volver a incluir el bias en esta formulación, se puede utilizar el truco de ampliar los ejemplares de entrada de vectores en $\xv \in \reals^d$ a vectores en $\xv' \in \reals^{d+1}$, donde $x_1=1$, y lo mismo con $\wv \in \reals^n$ a  $\wv' \in \reals^{n+1}$. 

\imagetwo{embeber_x}{0.5}{Conversión del ejemplar $\xv \in \reals^d$ a $\xv' \in \reals^{d+1}$, con $x_1=1$.}{embeber_w}{0.5}{Conversión del vector normal $\wv \in \reals^d$ a $\wv' \in \reals^{d+1}$, con $w_1=b$.}

De esta manera el componente $\wv_1$ representa el bias $b$, es decir, $\wv_1=b$, y así se puede mantener la misma regla de actualización, ya que:

\begin{equation}
 h(\xv')= \wv' \cdot \xv' = (b,\wv) \cdot (1,\xv) = b + \wv \cdot \xv
 \end{equation}

En lo siguiente se hará referencia $\wv'$ y $\xv'$ simplemente como $\wv$ y $\xv$, tomando atención a que son ahora vectores de $\reals^{d+1}$.

\subsubsection{Algoritmo clásico del Perceptrón}
El algoritmo tradicional del Perceptrón es, entonces:

\begin{algorithm}[H]
\KwData{Un conjunto de ejemplares $\xi \in D$, $D \subset \reals^{d+1}$, donde a cada $\xi \in \reals^d$ original se le agregó un 1 con el objetivo descripto en el párrafo anterior\\
Una clase $y_i \in \{-1,1\}$ asociada a cada $\xi \in D$ \\
Una tasa de aprendizaje $\alpha$. }
\KwResult{Un hiperplano descripto por $\wv \in \reals^{d+1}$}
$\wv \leftarrow random(d+1)$\;
\While{ $\exists \xi \in D: f(\xi) \neq y_i$ }{ 
  \For{$\xi \in D$}{
      \If{$f(\xi) \neq y_i$ }{
        $\wv \leftarrow \wv +  \alpha y_i \xi$ \;
      }  
  }    
}
\caption{Algoritmo de aprendizaje del Perceptrón} 
\end{algorithm}
\vspace{10pt}


Por último, el Perceptrón es un tipo de clasificador que particiona el dominio $\ddp$ en dos; clase $+1$ y clase $-1$. Entonces, existen solo dos regiones de voronoi, que cubren todo el espacio. Esto tiene la consecuencia de que el Perceptrón, un clasificador lineal, solo puede indicar si un ejemplar es de una clase u otra, pero no puede indicar si el mismo no es de ninguna clase. Entonces, en ocasiones, se debe complejizar la superficie utilizada para separar las clases para poder modelar su distribución en el dominio.

\subsubsection{Separabilidad lineal y Superficies de separación}

En la formulación anterior del algoritmo del Perceptrón hay una expectativa de que, ejecutando este procedimiento una cantidad finita de pasos, se encuentre un hiperplano que divida los dos conjuntos. Según el teorema de convergencia del Perceptrón \cite{haykin1994}, esto está garantizado  \textit{siempre y cuando} $D$ sea \textbf{linealmente separable}. Un conjunto de ejemplares pertenecientes a dos clases es linealmente separable justamente cuando existe un hiperplano $\wv \cdot \xv +b=0$ que puede separar los ejemplares de una clase de la otra.


\tikzimagetwo{clases_linealmente_separables}{scale=0.25}{Dos clases linealmente separables}{clases_no_linealmente_separables}{scale=0.25}{Dos clases que no son linealmente separables}

En la práctica, la gran mayoría de los dominios está compuesto por problemas cuyos ejemplares no son linealmente separables. En dichos casos existen dos alternativas. La primera, entrenar un clasificador que no utilice hiperplanos y emplee algún método de clasificación no lineal. El hiperplano es un tipo de \textbf{superficie de separación}. Un clasificador no lineal emplea una superficie de separación no-lineal, que aumenta el poder de clasificación porque resulta más flexible que un hiperplano.

\tikzimage{clases_no_linealmente_separables_con_superficie}{scale=0.25}{
Un clasificador que utiliza una superficie de separación no lineal $y=a x^3$.}

La segunda es aplicar una \textbf{transformación no-lineal} del espacio de entrada a otro espacio en donde los ejemplares si sean linealmente separables. En el problema clásico del \textbf{xor}, hay un conjunto de datos que no se puede separar con un Perceptrón. Si se aplica una transformación no lineal, a un espacio de la misma dimensión, se pueden obtener nuevas representaciones de los ejemplares de entrada que si son linealmente separables. Esto es equivalente en muchos casos a tener una superficie no-lineal de separación, pero con la simpleza de trabajar con un hiperplano en el clasificador.

Por este motivo, el concepto de separabilidad lineal sigue siendo extremadamente útil en todo tipo de problemas.

En la siguiente figura, la imagen a) muestra ejemplares de dos clases, triángulo y círculo, que no son linealmente separables ya que no existe un hiperplano que ponga de un lado los triángulos y de otro lado los círculos. La imagen b)
muestra un esquema de la función de transformación del espacio descripto en a). Esta es una transformación no-lineal basada en mezclas gaussianas, con 2 centros en los elementos de la clase triángulo. La función de transformación es $ \phi(\xv)= (e^{-( \norm{\xv -(1,1)}} , e^{-( \norm{\xv-(0,0)}} )$.  Finalmente, la imagen c) muestra el espacio transformado, donde ahora el conjunto de datos es linealmente separable. Los ejemplares de la clase triángulo se transformaron en los puntos $(1,\sqrt{2})$ y $(\sqrt{2},1)$, y los de la clase círculo en $(1,1)$. 

\tikzimagethree{transformacion_no_lineal_dominio}{scale=0.7}{El problema del xor.}
{transformacion_no_lineal_dominio_centros}{scale=0.7}{Esquema de las funciones de transformación.}
{transformacion_no_lineal_imagen}{scale=0.7}{Espacio transformado.}

%\subsubsection{Otra perspectiva sobre el bias $b$.}
%
%En este contexto, se puede considerar ver el truco de embeber los ejemplares en $\realdp$ como una transformación no lineal. La función $h$ define un hiperplano afín en $\reald$, ya que la función $h$ es afín en $\reald$. El término $b$ es necesario para aumentar el poder clasificador del Perceptrón, ya que ciertos conjuntos de ejemplares no son separables con un hiperplano lineal pero si con uno afín, y si $b \neq 0$, $h$ define un hiperplano afín.
%
%
%Pero en realidad $h$, definida con el truco de embeber los ejemplares en $\realdp$, representa un hiperplano lineal en $\realdp$, y esta transformación no-lineal en los ejemplares a un espacio de mayor dimensionalidad permite que sean separados. Es debido a esta simple equivalencia que se habla del perceptrón como un \textit{clasificador lineal}, cuya definición involucra en realidad un hiperplano afín.
%
%La transformación $\fdef{\phi}{\ddp}{\ddp'}$ es una \textbf{característica} de los ejemplares, otra representación de los mismos que es más amena para clasificar. 

\subsubsection{Clasificación aproximada y medidas de error}

En la gran mayoría de casos, resulta imposible clasificar perfectamente un conjunto de datos. Se puede proponer entonces un algoritmo modificado de entrenamiento del perceptron que no se detenga cuando todos los ejemplares de entrenamiento están clasificados correctamente, sino cuando el error cometido por el Perceptrón es lo suficientemente bajo\footnote{Se está asumiendo que el error puede llevarse a un valor menor que la tolerancia $\epsilon$. En casos prácticos no se puede saber esto de antemano, por ende se suele agregar un contador de iteraciones $t$ al algoritmo, estableciendo previamente una cantidad de iteraciones máximas en las que se intentará disminuir el error.}. Para ello, se asumirá que existe una función $\rho(D,f)$ que mide el error del clasificador $f$ en el conjunto de ejemplares $D$, de alguna manera.

\begin{algorithm}[H]
\KwData{ \\
Un conjunto de ejemplares $D \in \reals^d$\\
Una clase $y_i \in \{-1,1\}$ asociada a cada $\xi \in D$ \\
Una tasa de aprendizaje $\alpha$.\\ 
Una tolerancia al error $\epsilon$\\
Una función que mide el error de clasificación $\rho$}
\KwResult{Un hiperplano descripto por $\wv \in \reals^{d+1}$}
$\wv \leftarrow random(d+1)$\;
$error \leftarrow \infty$\;
\While{ $error > \epsilon $ }{ 
  \For{$\xi \in D$}{
      \If{$f(\xi) \neq y_i$ }{
        $\wv \leftarrow \wv +  \alpha y_i \xi$ \;
      }  
  }
  $error =  \rho(D,f)$\;
}
\caption{Algoritmo de aprendizaje del Perceptrón con tolerancia al error.} 
\end{algorithm}
\vspace{10pt}

Como caso especial, si $\epsilon=0$, se vuelve a exigir que el entrenamiento continúe hasta que se separen perfectamente todos los ejemplares.

Se puede definir a $\rho$ como dependiente de una función de pérdida $\loss$, que mide el error para un ejemplar $\xv$ en particular, en base a su clase, $y$, y a la salida del clasificador, $f(\xv)$.  

Como ejemplo, la función de error $\rho$ más general es el porcentaje de ejemplares mal clasificados. En ese caso, la función de pérdida $\loss_0$ es $1$ si el ejemplar está mal clasificado y $0$ de lo contrario:

\ma{
\loss_0(\xv	,f) &= 1-\delta(y,f(\xv))
}
donde $y$ es la clase de $\xv$ y $\delta$ es el delta de Kronecker $\delta(a,b)= \caseotherwise{1}{0}{a=b}$. 

En base a eso, se define la función $\rlz$ que utiliza la función $\lossz$ para medir el error como el porcentaje de ejemplares mal clasificados:
\ma{
\rlz(D,f) &= \frac{\sum\limits_{\xi \in D} \lossz(\xi,f) }{|D|} \\
}

Existen varias funciones $\rho$ y $\loss$ que se podrían definir para medir el error de clasificación, y se presentarán otras en las siguientes secciones. 