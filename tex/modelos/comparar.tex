

\subsection{Support Vector Machine (SVM)}

Utilizando con la característica $\phi_d$ directamente, se puede pensar en un gesto como un punto en un espacio n-dimensional de direcciones. El clasificador toma toda la secuencia de direcciones y genera un modelo que distingue gestos.

En el caso de SVM, se asume una función de distancia definida entre puntos o gestos de dicho espacio. Dos puntos cercanos en el espacio, en base de esa medida de distancia, se consideran gestos parecidos. Dadas $C$ clases, si se entrenan $C$ clasificadores para que distingan los ejemplares de cada clase de los ejemplares de las otras, se encuentran $C$ superficies que separen dichas clases; estas dependen del kernel elegido. Se utilizaron dos kernels en los experimentos: el lineal y el gaussiano. 

En el caso del lineal, se busca simplemente un hiperplano separador. En el caso del kernel gaussiano,  la fórmula resultante de $f$ es:

\ma{
f(\xv) &= \sumi \ai y_i \kn{\xi}{\xv} +b \\
&= \sumi \ai y_i e^{-\norm{\xi-\xv}/\sigma} +b}

donde $\sigma$ es un parámetro de desviación estándar, $\norm{\cdot}$ es la norma euclídea, $y_i$ es la clase del gesto de entrenamiento $i$, y $\ai$ es su variable dual. Entonces, se puede interpretar esta fórmula de la siguiente manera. 

Cada función gaussiana es una función que calcula la similitud entre el ejemplar $\xv$ y el ejemplar de entrenamiento $\xi$, escalado por $\sigma$. En el mejor caso $\xi=\xv$, por ende $e^{-\norm{\xi-\xv}/\sigma}=1$ y entonces, el término i-ésimo de la sumatoria es $\ai y_i$. A medida que $\xv$ es cada vez más diferente de $\xi$, $e^{-\norm{\xi-\xv}/\sigma}$ tiende a 0, y entonces dicho término prácticamente se anula en la sumatoria.  

Como $\ai\geq 0 $ y $y_i= \pm 1$, se puede interpretar esto como que se seleccionan los vectores del conjunto de entrenamiento más cercanos a $\xv$, pesados por la distancia entre $\xv$ y $\xi$, y además $\ai$, y se los suma en la dirección en que apunta $y_i$. Si $y_i=1$ el término i-ésimo "vota" para que el vector $\xv$ sea considerado de clase $1$, con una fuerza de votación $\ai  e^{-\norm{\xi-\xv}/\sigma}$, y viceversa si $y_i=-1$. 

Se utiliza la versión isotrópica del kernel gaussiano, $e^{-\norm{\xi-\xv}/\sigma}$, que difiere de la versión general en que no se incluye la matriz de covarianza bajo la común hipótesis de que los ejemplares pueden tomarse distribuidos isotrópicamente en el espacio ( o sea, la matriz de covarianza es la identidad).

Por último, $b$ ajusta la línea de corte, por las mismas razones mencionadas en el capítulo \ref{chap:aprendizaje} para agregar un bias en el perceptrón. En este caso, $\xv$ y $\xi$ son gestos; el kernel gaussiano entonces implementaría la noción del capítulo \ref{chap:gestos} de una equivalencia aproximada $\eequiv$ entre gestos.

\subsection{Redes Neuronales Feedforward (FF)}

Para las redes neuronales feedforward, así como para SVM, se utiliza el vector de características de cada gesto completo como si fuese un punto en un espacio n-dimensional de gestos.

De todos modos, el vector de direcciones no es simplemente un conjunto de datos, sino que tiene un orden y estructura particular, ya que representa una secuencia ordenada de direcciones, donde bajo la hipótesis de continuidad, cada dirección está correlacionada con la anterior. 

Esta correlación se puede modelar utilizando los modelos de redes neuronales recurrentes, pero también utilizando el modelo \textbf{input-delay} \cite{Haykin1998}, en donde se alimenta a la red con \textit{todo} el vector de direcciones, y se espera que la red misma modele las dinámicas de las secuencias de direcciones. Este es el enfoque que aquí se toma; la red toma un vector de $n$ direcciones, y por ende utiliza $3 \times n$ neuronas de entrada. Se utiliza una sola capa oculta con $h$ neuronas, con $h$ determinado experimentalmente. La capa de salida tiene tantas neuronas como clases, $C$; cada salida $o_c$ representa la confianza de que el ejemplar $\xv$ pertenezca a la clase $c$.
 

\subsection{Clasificador basado en templates}
\chaptertexinput{gmm}