%
%The Ordered Means Model (OMM) is a simplified Hidden Markov Model (HMM) which has been successfully applied for solving gesture recognition problems in a variety of settings. Moreover, HMMs are the de-facto standard for generative classifier models, (and, arguably, gesture recognition \cite{Just2009}) and thus a good choice for comparison. 
%
%While most model building methods for both the OMM and HMM commonly employ some variant of the Expectation-Maximization (EM) algorithm to find optimal values for their parameters, such a choice is ill-suited for small training sets as required for our problem. Therefore, we have created the Small-Training-Set-OMM (ST-OMM), an adaptation of the OMM approach to comply with this requirement. 
%
%
\subsubsection{Arquitectura y entrenamiento}
%
%The ST-OMM takes as input a sample, which is a fixed length sequence $\ve{s}$ of $n$ features, and outputs the likelihoods of the sample belonging to each gesture class $c$.  The ST-OMM is built with $n$ competitive Gaussian Mixture Models (C-GMM), and each C-GMM $G_j, j=1 \dots n$ is composed of a set of states $S_{j,c},c \in C$, with constant mixture coefficients $\omega_c=1/|C|$. To every state $S_{j,c}$ corresponds to a set of Gaussian pdfs with means $\gu$ and covariance matrices $\gc, k=1 \dots |Sc|$, where $Sc$ is the set of samples of class $c$, which models the probability of an emission of gesture part $j$ by the gesture class $c$ in a competitive fashion. This parameters are estimated as $\gu = \sck [j] $ and $\gc = cov(I_{j,c}) $, $ k= 1  \dots |Sc|$, where $\sck$ is the $kth$ sample of class $c$ and $I_{j,c}$ is a matrix whose columns are $\begin{bsmallmatrix} \scj{1} & \scj{2} &  \dots & \scj{|Sc|}  & k= 1  \dots |Sc| \end{bsmallmatrix}$, that is, $I_{j,c}$ is the matrix that contains the $j^{\text{\tiny th}}$ feature of every sample of class $c$.   
%
%We are therefore using each part of every sample of the training set - the best likelihood estimators for that set - as a C-GMM mean, which does not yield a computationally demanding model because we are specifically targeting a very small training sets. 
%For the classification of a new sample $\ve{s}$, we calculate the likelihood of emission for each state $\stcj$ as:
%
%\begin{alignat*}{3}
%P(\stcj | s[j])= \frac{ P(s[j] | \stcj) P(\stcj)}{P(s[j])} &= 
%\frac{ p_{j,c} P(\stcj)}{\sum_{k \in C} p_{j,k} P(S_{j,k})} &=
%\frac{ p_{j,c}}{\sum_{k \in C} p_{j,k}}
%\end{alignat*}
%where $P(S_{j,k}) = \frac{1}{|C|}$ is the same for all classes $k$, and $ p_{j,k} = P(s[j]|\stcj) = max({\normal(s[j];\gu,\gc)}) $ is the maximum of the scores that model the likelihood of the $j^{\text{\tiny th}}$ feature of the sample belonging to class $c$.
%
%Then, the likelihood of the whole sample belonging to class $c$ is:
%
%\begin{alignat*}{3}
%P(c | s) &= 
%\frac{P(s | c) P(c)}{\sum_{k \in C} P(s | k) P(k)} &=
%\frac{P(s | c) }{\sum_{k \in C} P(s | k) }
%\end{alignat*}
%
%where we define $P(k)= \frac{1}{|C|} $ and $P(s | k) = \frac{\sum_{j=1}^n P (S_{j,k} |s[j])}{n}$
%
%We can thus picture the ST-OMM as a $|C| \times n$ state HMM, with one left-to-right submodel for each gesture that does not allow a transition from a state to itself, that is:
%\begin{align*}
%P(\stcj \rightarrow S_{j',c}) =
  %\begin{cases} 
      %1 & \textrm{ if $j'=j+1$} \\
      %0 & \textrm{ otherwise} \\
   %\end{cases} 
%\end{align*}
%
%This restriction avoids doing a dynamic programming search over state combinations and, although it is obviously of lower computational capacity than a full HMM, works well given a large enough $n$, since any desynchronization between a novel performance of the gesture and the model produces only small mismatches.
%
 %
%
