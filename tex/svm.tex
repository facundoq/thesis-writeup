


\epigraph{Why is it so popular? I believe there are several reasons. First of all, it is effective. It gives very stable and good results. From a theoretical point of view, it is very clear, very simple, and allows many different generalizations, called kernel machines. }{Vladimir N. Vapnik}


\section{Introducción}
El separador de espacios por excelencia es el hiperplano. Pero dado un conjunto de datos con dos clases linealmente separables, pueden existir infinitos hiperplanos que separan los conjuntos. 

\tikzimage{hiperplano_svm_varios}{scale=0.25}{Distintos hiperplanos separadores para el mismo conjunto de datos.}

Algoritmos simples como el del Perceptrón se quedan con el primero que encuentran que tenga un error lo suficientemente bajo, ya que no tienen un criterio para elegir entre todos los posibles hiperplanos que separan las clases. Si bien en la figura anterior la distribución real de los ejemplos de las clases no se conoce, y por ende no se puede asegurar cual es el hiperplano más correcto, está claro que, a priori, algunos hiperplanos capturan mejor que otros la división entre las dos clases. 

Las \textbf{Máquinas de Vectores de Soporte} (SVM) extienden el perceptrón con un criterio para seleccionar entre los hiperplanos; en este modelo, se busca el \textit{hiperplano de máximo margen}.

\section{Clasificador de Márgen Máximo}
\texinput{svm/margen}

\section{Modelo de Márgenes Suaves}
\texinput{svm/soft}

\section{Forma Dual}
\texinput{svm/dual}

\section{Análisis de los valores de $\ai$}
\texinput{svm/analisis}

\section{El truco del Kernel}
\texinput{svm/kernel}

%\section{Algoritmo de entrenamiento SMO}
%\texinput{svm/smo}

\section{Adaptación para el reconocimiento multiclase}
\texinput{svm/multiclase}

\section{Resumen}
\texinput{svm/resumen}

